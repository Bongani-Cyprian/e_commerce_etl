name: CI/CD for Airflow ETL

on:
  pull_request:
    branches: [main]
  push:
    branches: [staging, main]

jobs:

  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Dump workspace
        run: |
          pwd
          ls -R .

      - uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install lint dependencies
        run: |
          pip install --upgrade pip
          pip install flake8 apache-airflow pandas google-cloud-bigquery
          pip install -r requirements.txt

      - name: Run flake8
        run: flake8 airflow/dags --max-line-length=88

  test:
    needs: lint
    runs-on: ubuntu-latest
    env:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW_HOME: ${{ github.workspace }}/airflow_home

    steps:
      - uses: actions/checkout@v3

      - name: Dump workspace
        run: |
          pwd
          ls -R .

      - name: Prepare AIRFLOW_HOME
        run: mkdir -p $AIRFLOW_HOME

      - uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install test dependencies
        run: |
          pip install --upgrade pip
          pip install apache-airflow pytest
          pip install -r requirements.txt

      - name: Initialize Airflow DB
        run: airflow db init

      - name: Run tests
        run: pytest -q

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Build Docker image
        run: docker build -t my-etl-image .

  push-image:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Authenticate with GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Configure Docker for GCR
        run: echo ${{ secrets.GCP_CREDENTIALS }} \
          | docker login -u _json_key --password-stdin https://gcr.io

      - name: Tag & push
        run: |
          IMAGE=gcr.io/${{ secrets.GCP_PROJECT_ID }}/my-etl-image:latest
          docker tag my-etl-image $IMAGE
          docker push $IMAGE

  deploy-gke:
    needs: push-image
    runs-on: ubuntu-latest
    env:
      GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
      GKE_CLUSTER: your-cluster-name
      GKE_ZONE: your-cluster-zone
      IMAGE: gcr.io/${{ secrets.GCP_PROJECT_ID }}/my-etl-image:latest

    steps:
      - uses: actions/checkout@v3

      - name: Authenticate with GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Set gcloud defaults
        run: |
          gcloud config set project $GCP_PROJECT_ID
          gcloud config set compute/zone $GKE_ZONE

      - name: Fetch GKE creds
        run: gcloud container clusters get-credentials $GKE_CLUSTER

      - name: Roll new image into deployments
        run: |
          kubectl set image deployment/etl-webserver etl-webserver=$IMAGE
          kubectl set image deployment/etl-scheduler etl-scheduler=$IMAGE

      - name: Wait for rollout
        run: |
          kubectl rollout status deployment/etl-webserver
          kubectl rollout status deployment/etl-scheduler

  trigger-dag:
    needs: deploy-gke
    runs-on: ubuntu-latest
    env:
      AIRFLOW_ENDPOINT: ${{ secrets.AIRFLOW_WEBSERVER_URL }}
      AIRFLOW_USER: ${{ secrets.AIRFLOW_USER }}
      AIRFLOW_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}
      DAG_ID: my_etl_dag

    steps:
      - name: Trigger DAG run via REST
        run: |
          curl -X POST "$AIRFLOW_ENDPOINT/api/v1/dags/$DAG_ID/dagRuns" \
            -u "$AIRFLOW_USER:$AIRFLOW_PASSWORD" \
            -H "Content-Type: application/json" \
            -d '{"conf":{}}'
