name: CI/CD for Airflow ETL

on:
  pull_request:
    branches: [main]
  push:
    branches: [staging, main]

defaults:
  run:
    shell: bash
    # fail fast on any error, undefined var, or pipe failure
    working-directory: ${{ github.workspace }}
    env:
      # common env for error-checking
      BASH_OPTS: "euo pipefail"

jobs:

  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Dump workspace
        run: |
          pwd
          ls -R .

      - uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install lint dependencies
        run: |
          pip install --upgrade pip
          pip install flake8 apache-airflow pandas google-cloud-bigquery
          pip install -r requirements.txt

      - name: Run flake8
        run: flake8 airflow/dags --max-line-length=88

  test:
    needs: lint
    runs-on: ubuntu-latest
    env:
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW_HOME: ${{ github.workspace }}/airflow_home
    steps:
      - uses: actions/checkout@v3

      - name: Dump workspace
        run: |
          pwd
          ls -R .

      - name: Prepare AIRFLOW_HOME
        run: mkdir -p "$AIRFLOW_HOME"

      - uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install test dependencies
        run: |
          pip install --upgrade pip
          pip install apache-airflow pytest
          pip install -r requirements.txt

      - name: Initialize Airflow DB
        run: |
          airflow db init

      - name: Run pytest
        run: pytest -q

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Build Docker image
        run: docker build -t my-etl-image:latest .

  smoke-test:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Launch container
        run: |
          docker run -d \
            --name etl-smoke \
            -p 8081:8081 \
            my-etl-image:latest

      - name: Wait for health endpoint
        run: |
          for i in {1..30}; do
            if curl --fail http://localhost:8081/health; then
              echo "✅ Webserver is healthy"
              break
            fi
            echo "Waiting for webserver (attempt $i)…"
            sleep 5
          done

      - name: Verify entrypoint script exists
        run: |
          docker exec etl-smoke ls -l /entrypoint.sh

      - name: Tear down smoke container
        run: docker stop etl-smoke

  push-image:
    needs: smoke-test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Authenticate with GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Configure Docker for GCR
        run: |
          echo "${{ secrets.GCP_CREDENTIALS }}" \
            | docker login -u _json_key --password-stdin https://gcr.io

      - name: Tag & push
        run: |
          IMAGE=gcr.io/${{ secrets.GCP_PROJECT_ID }}/my-etl-image:latest
          docker tag my-etl-image:latest "$IMAGE"
          docker push "$IMAGE"

  deploy-gke:
    needs: push-image
    runs-on: ubuntu-latest
    env:
      GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
      GKE_CLUSTER: your-cluster-name
      GKE_ZONE: your-cluster-zone
      IMAGE: gcr.io/${{ secrets.GCP_PROJECT_ID }}/my-etl-image:latest
    steps:
      - uses: actions/checkout@v3

      - name: Authenticate with GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Set gcloud defaults
        run: |
          gcloud config set project "$GCP_PROJECT_ID"
          gcloud config set compute/zone "$GKE_ZONE"

      - name: Fetch GKE credentials
        run: gcloud container clusters get-credentials "$GKE_CLUSTER"

      - name: Update deployments
        run: |
          kubectl set image deployment/etl-webserver etl-webserver="$IMAGE"
          kubectl set image deployment/etl-scheduler etl-scheduler="$IMAGE"

      - name: Wait for rollout
        run: |
          kubectl rollout status deployment/etl-webserver
          kubectl rollout status deployment/etl-scheduler

  trigger-dag:
    needs: deploy-gke
    runs-on: ubuntu-latest
    env:
      AIRFLOW_ENDPOINT: ${{ secrets.AIRFLOW_WEBSERVER_URL }}
      AIRFLOW_USER: ${{ secrets.AIRFLOW_USER }}
      AIRFLOW_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}
      DAG_ID: my_etl_dag
    steps:
      - name: Trigger DAG run via REST
        run: |
          curl -X POST "$AIRFLOW_ENDPOINT/api/v1/dags/$DAG_ID/dagRuns" \
            -u "$AIRFLOW_USER:$AIRFLOW_PASSWORD" \
            -H "Content-Type: application/json" \
            -d '{"conf":{}}'
